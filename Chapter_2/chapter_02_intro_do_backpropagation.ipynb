{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thevirusoup/Neural-Networks/blob/main/Chapter_2/chapter_02_intro_do_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e05635-45e7-4d10-9dd1-00ec79213906",
      "metadata": {
        "id": "80e05635-45e7-4d10-9dd1-00ec79213906"
      },
      "source": [
        "# The StatQuest Illustrated Guide to Neural Networks and AI\n",
        "## Chapter 2 - Build and train a very simple neural network with backpropagation\n",
        "\n",
        "Copyright 2024, Joshua Starmer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a3466e-96aa-47bb-a7af-b8e9a93d3d5d",
      "metadata": {
        "id": "61a3466e-96aa-47bb-a7af-b8e9a93d3d5d"
      },
      "source": [
        "In this notebook, we will build, and train the very simple neural network featured in Chapter 2 of **[The StatQuest Illustrated Guide to Neural Networks and AI](https://www.amazon.com/dp/B0DRS71QVQ)**. We'll start with the untrained Weights and Biases, as seen in the figure below...\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_2_train_all.png?raw=1\" alt=\"an untrained neural network\" style=\"width: 800px;\">\n",
        "\n",
        "...and then train them with Backpropagation to, hopefully, get the trained Weights and Biases seen in the figure below.\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_1_pre_trained_nn_labeled.png?raw=1\" alt=\"a trained neural network\" style=\"width: 800px;\">\n",
        "\n",
        "In this tutorial, you will...\n",
        "\n",
        "- **[Code a neural network with untrained wieghts and biases](#untrained)** This will show the basic structure of a class that inherits from `LightningModule` to build a neural network.\n",
        "\n",
        "- **[Train the weights and biases in our neural network](#train)** This will show how to train the weights and biases in a simple neural network.\n",
        "\n",
        "#### NOTE:\n",
        "This tutorial assumes that you already know the basics of coding in **Python** and have read the first two chapters in **The StatQuest Illustrated Guide to Neural Networks and AI**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79033cc7-bcd6-409c-842f-ed7bac55c687",
      "metadata": {
        "id": "79033cc7-bcd6-409c-842f-ed7bac55c687"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252a97a7-ba0d-449a-8c1f-551d468e6038",
      "metadata": {
        "id": "252a97a7-ba0d-449a-8c1f-551d468e6038"
      },
      "source": [
        "# Import the modules that will do all the work\n",
        "\n",
        "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b9885b-59a2-4148-ad19-0c822471a360",
      "metadata": {
        "id": "c7b9885b-59a2-4148-ad19-0c822471a360"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
        "\n",
        "## NOTE: If you **don't** need to install anything, you can comment out the\n",
        "##       next line.\n",
        "##\n",
        "##       If you **do** need to install something, just know that you may need to\n",
        "##       restart your session for python to find the new module(s).\n",
        "##\n",
        "##       To restart your session:\n",
        "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
        "##         \"Restart Session\" from the pulldown menu\n",
        "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
        "##         \"Restart Kernel\" from the pulldown menu\n",
        "##\n",
        "##       Also, installing can take a few minutes, so go get yourself a snack!\n",
        "!pip install lightning seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c818152-6ca2-4ee0-9168-f77c9b4cac1c",
      "metadata": {
        "id": "8c818152-6ca2-4ee0-9168-f77c9b4cac1c"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us relu()\n",
        "from torch.optim import SGD # SGD is short of Stochastic Gradient Descent, but\n",
        "                            # the way we'll use it, passing in all of the training\n",
        "                            # data at once instead of passing it random subsets,\n",
        "                            # it will act just like plain old Gradient Descent.\n",
        "\n",
        "import lightning as L ## Lightning makes it easier to write, optimize and scale our code\n",
        "from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n",
        "\n",
        "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
        "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.\n",
        "\n",
        "## NOTE: If you get an error running this block of code, it is probably\n",
        "##       because you installed a new package earlier and forgot to\n",
        "##       restart your session for python to find the new module(s).\n",
        "##\n",
        "##       To restart your session:\n",
        "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
        "##         \"Restart Session\" from the pulldown menu\n",
        "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
        "##         \"Restart Kernel\" from the pulldown menu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c06585c7-4310-4f59-8c22-55f53a17e905",
      "metadata": {
        "id": "c06585c7-4310-4f59-8c22-55f53a17e905"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ca5fb5-e879-4453-9e86-dbc37c158afa",
      "metadata": {
        "id": "b4ca5fb5-e879-4453-9e86-dbc37c158afa"
      },
      "source": [
        "# Create the Training Dataset\n",
        "\n",
        "In Chapter 2, we had a very simple dataset that consisted of three points, as seen in the figure below.\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_2_training_data.png?raw=1\" alt=\"a simple dataset for training\" style=\"width: 800px;\">\n",
        "\n",
        "Although it's not required, we're going to put our training data into a `DataLoader`. `DataLoaders` are easy to make and they offer a lot of cool features. For example, if we had a large dataset, a `DataLoader` gives us a super easy way to access the data in batches instead of all at once. This is critical when we have more data than RAM to store it in. A DataLoader can also shuffle the data for us each epoch and makes it easy to only use a fraction of the data if we want to do a quick and rough training for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49840acf-ddb6-4577-8401-de872ab62c32",
      "metadata": {
        "id": "49840acf-ddb6-4577-8401-de872ab62c32"
      },
      "outputs": [],
      "source": [
        "## The inputs are the x-axis coordinates for each data point\n",
        "## These values represent different doses\n",
        "training_inputs = torch.tensor([0.0, 0.5, 1.0])\n",
        "\n",
        "## The labels are the y-axis coordinates for each data point\n",
        "## These values represent the effectiveness\n",
        "training_labels = torch.tensor([0.0, 1.0, 0.0])\n",
        "\n",
        "## Now let's package everything up into a DataLoader...\n",
        "training_dataset = TensorDataset(training_inputs, training_labels)\n",
        "dataloader = DataLoader(training_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d183574-134b-4be3-9fd5-186a3a946d9b",
      "metadata": {
        "id": "7d183574-134b-4be3-9fd5-186a3a946d9b"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adfb2eb-0ad7-426d-9946-6a7185737647",
      "metadata": {
        "id": "7adfb2eb-0ad7-426d-9946-6a7185737647"
      },
      "source": [
        "# Create a Neural Network with Trainable Weights and Biases\n",
        "<a id=\"create\"></a>\n",
        "\n",
        "Now we'll build a neural network that has trainable Weights and Biases. For this, we'll use `L.LightningModule`, which has everything `nn.Module` has, plus we can define the optimizer we want to use as well as tell PyTorch how each training step should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4881afee-1cd9-4440-8ffc-1f86b321c539",
      "metadata": {
        "id": "4881afee-1cd9-4440-8ffc-1f86b321c539"
      },
      "outputs": [],
      "source": [
        "class myNN(L.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## Create all of the weights and biases for the network.\n",
        "        ## However, this time they are initialized with random values.\n",
        "        ## We are also wrapping the tensors up in nn.Parameter() objects.\n",
        "        ## PyTorch will only optimize parameters. There are a lot of\n",
        "        ## different ways to create parameters, and we'll see those\n",
        "        ## in later examples, but nn.Parameter() is the most basic.\n",
        "        self.w1 = nn.Parameter(torch.tensor(0.06))\n",
        "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "        self.w2 = nn.Parameter(torch.tensor(3.49))\n",
        "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "        self.w3 = nn.Parameter(torch.tensor(-4.11))\n",
        "        self.w4 = nn.Parameter(torch.tensor(2.74))\n",
        "\n",
        "        self.loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        ## The forward() method is identical to what we used in Chapter 1.\n",
        "\n",
        "        top_x_axis_values = (input_values * self.w1) + self.b1\n",
        "        bottom_x_axis_values = (input_values * self.w2) + self.b2\n",
        "\n",
        "        top_y_axis_values = F.relu(top_x_axis_values)\n",
        "        bottom_y_axis_values = F.relu(bottom_x_axis_values)\n",
        "\n",
        "        output_values = (top_y_axis_values * self.w3) + (bottom_y_axis_values * self.w4)\n",
        "\n",
        "        return output_values\n",
        "\n",
        "\n",
        "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
        "        return SGD(self.parameters(), lr=0.01)\n",
        "        ## NOTE: PyTorch doesn't have a Gradient Descent optimizer, just a\n",
        "        ## Stochastic Gradient Descent (SGD) optimizer. However, since we\n",
        "        ## are running all 3 doses through the NN each time, rather than a\n",
        "        ## random subset, we are essentially doing Gradient Descent instead of\n",
        "        ## SGD.\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
        "        ## NOTE: When training_step() is called it calculates the loss with the code below...\n",
        "        inputs, labels = batch # collect input\n",
        "        outputs = self.forward(inputs) # run input through the neural network\n",
        "        loss = self.loss(outputs, labels) ## the `loss` quantifies the difference between\n",
        "                                          ## the observed drug effectiveness in `labels`\n",
        "                                          ## and the outputs created by the neural network\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff36d865-fcab-435d-9b1d-484a3ed8cd7a",
      "metadata": {
        "id": "ff36d865-fcab-435d-9b1d-484a3ed8cd7a"
      },
      "outputs": [],
      "source": [
        "model = myNN() # First, make model from the class\n",
        "\n",
        "## Now print out the name and value for each named parameter\n",
        "## parameter in the model. Remember parameters are variables,\n",
        "## like Weights and Biases, that we can train.\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, torch.round(param.data, decimals=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd1842d-b97c-4b8f-91c9-6bc6884e1dfa",
      "metadata": {
        "id": "9bd1842d-b97c-4b8f-91c9-6bc6884e1dfa"
      },
      "outputs": [],
      "source": [
        "## now run different doses through the neural network.\n",
        "output_values = model(training_inputs)\n",
        "torch.round(output_values, decimals=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c0a5a5-47f0-4e45-be2b-a7cf10756e29",
      "metadata": {
        "id": "e0c0a5a5-47f0-4e45-be2b-a7cf10756e29"
      },
      "source": [
        "# BAM!\n",
        "\n",
        "We successfully ran the doses from the training data through the model. However, the output from the model is way different than we expect (we expected 0.0, 1.0, and 0.0). So let's draw a picture of the bent shape that the model uses to make predictions and compare that to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901253af-43f6-40dc-97af-175c36e5290e",
      "metadata": {
        "id": "901253af-43f6-40dc-97af-175c36e5290e"
      },
      "outputs": [],
      "source": [
        "## Create the different doses we want to run through the neural network.\n",
        "## torch.linspace() creates the sequence of numbers between, and including, 0 and 1.\n",
        "input_doses = torch.linspace(start=0, end=1, steps=11)\n",
        "\n",
        "# now print out the doses to make sure they are what we expect...\n",
        "input_doses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84cbadf-86ec-4f11-af2c-dfece12f197a",
      "metadata": {
        "id": "c84cbadf-86ec-4f11-af2c-dfece12f197a"
      },
      "outputs": [],
      "source": [
        "output_values = model(input_doses)\n",
        "output_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3bc01f1-65d4-4fb0-b428-6730f64d829a",
      "metadata": {
        "id": "f3bc01f1-65d4-4fb0-b428-6730f64d829a"
      },
      "outputs": [],
      "source": [
        "## Now draw a graph that shows how well, or poorly, the model\n",
        "## predicts the training data. At this point, since the\n",
        "## model is untrained, there should be a big difference between\n",
        "## the model's output and the training data.\n",
        "\n",
        "## First, set the style for seaborn so that the graph looks cool.\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "## First, draw the individual output points\n",
        "sns.scatterplot(x=input_doses,\n",
        "                y=output_values.detach().numpy(),\n",
        "                color='green',\n",
        "                s=200)\n",
        "\n",
        "## Now connect those points with a line\n",
        "sns.lineplot(x=input_doses,\n",
        "             y=output_values.detach().numpy(), ## NOTE: We call .detatch() because...\n",
        "             color='green',\n",
        "             linewidth=2.5)\n",
        "\n",
        "## Add the values in the training dataset\n",
        "sns.scatterplot(x=training_inputs,\n",
        "                y=training_labels,\n",
        "                color='orange',\n",
        "                s=200)\n",
        "\n",
        "## now label the y- and x-axes.\n",
        "plt.ylabel('Effectiveness')\n",
        "plt.xlabel('Dose')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b10eaf-4fae-46bc-af6c-f05ebdada746",
      "metadata": {
        "id": "89b10eaf-4fae-46bc-af6c-f05ebdada746"
      },
      "source": [
        "# DOUBLE BAM!!\n",
        "\n",
        "Now that we see how badly the bent shape fits the training data, let's train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab161cb5-f837-42b1-b47b-1a4af10b2169",
      "metadata": {
        "id": "ab161cb5-f837-42b1-b47b-1a4af10b2169"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59123d80-2a03-4f66-9a41-f3e85b040ddd",
      "metadata": {
        "id": "59123d80-2a03-4f66-9a41-f3e85b040ddd"
      },
      "source": [
        "# Training the Weights and Biases in the Neural Network\n",
        "\n",
        "Training consists of creating a **Lightning Trainer** with `L.Trainer()` and then calling the `fit()` method on the our model with the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033b9bc6-39ac-4fb1-aabd-355f22f54677",
      "metadata": {
        "id": "033b9bc6-39ac-4fb1-aabd-355f22f54677"
      },
      "outputs": [],
      "source": [
        "model = myNN()\n",
        "## Now train the model...\n",
        "trainer = L.Trainer(max_epochs=500, # how many times to go through the training data\n",
        "                    logger=False,\n",
        "                    enable_checkpointing=False,\n",
        "                    enable_progress_bar=False)\n",
        "\n",
        "trainer.fit(model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146b0cf3-9541-4d59-853f-63202fbad7ae",
      "metadata": {
        "id": "146b0cf3-9541-4d59-853f-63202fbad7ae"
      },
      "outputs": [],
      "source": [
        "## Now that we've trained the model, let's print out the\n",
        "## new values for each Weight and Bias.\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, torch.round(param.data, decimals=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1935b5-4256-49c9-9117-8aab8ad382b3",
      "metadata": {
        "id": "bd1935b5-4256-49c9-9117-8aab8ad382b3"
      },
      "source": [
        "Lastly, let's draw a graph of the bent shape that the model is using for predictions and compare it to the training data. In theory, the bent shape should fit the data much better now that we have optimized the Weights and Biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11539eec-29cc-4efb-ab2e-ddf82ba360c2",
      "metadata": {
        "id": "11539eec-29cc-4efb-ab2e-ddf82ba360c2"
      },
      "outputs": [],
      "source": [
        "## now run the different doses through the neural network.\n",
        "output_values = model(input_doses)\n",
        "torch.round(output_values, decimals=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186b2d67-8621-4be3-937e-e1fc980475a9",
      "metadata": {
        "id": "186b2d67-8621-4be3-937e-e1fc980475a9"
      },
      "outputs": [],
      "source": [
        "## Now draw a graph that shows how well, or poorly, the model\n",
        "## predicts the training data. At this point, since we just\n",
        "## trained th model, the training data should overlap the\n",
        "## model's output\n",
        "\n",
        "## First, set the style for seaborn so that the graph looks cool.\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "## First, draw the individual output points\n",
        "sns.scatterplot(x=input_doses,\n",
        "                y=output_values.detach().numpy(),\n",
        "                color='green',\n",
        "                s=200)\n",
        "\n",
        "## Now connect those points with a line\n",
        "sns.lineplot(x=input_doses,\n",
        "             y=output_values.detach().numpy(), ## NOTE: We call .detatch() because...\n",
        "             color='green',\n",
        "             linewidth=2.5)\n",
        "\n",
        "## Add the values in the training dataset\n",
        "sns.scatterplot(x=training_inputs,\n",
        "                y=training_labels,\n",
        "                color='orange',\n",
        "                s=200)\n",
        "\n",
        "## now label the y- and x-axes.\n",
        "plt.ylabel('Effectiveness')\n",
        "plt.xlabel('Dose')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6a835c4-5041-4974-987f-9c17b8396908",
      "metadata": {
        "id": "b6a835c4-5041-4974-987f-9c17b8396908"
      },
      "source": [
        "# TRIPLE BAM!!!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}